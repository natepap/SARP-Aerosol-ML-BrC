{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = 'C:/Stuff/Projects/SARP-Aerosol-ML-BrC/Data/'\n",
    "inputDir = rootDir + 'Processed/'\n",
    "networkPath = rootDir + 'Network/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(inputDir+'input')\n",
    "y = pd.read_csv(inputDir+'output')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.67, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1928306316.00530028\n",
      "Iteration 2, loss = 344429436.22093213\n",
      "Iteration 3, loss = 2976229241.07398844\n",
      "Iteration 4, loss = 7087307329.60097599\n",
      "Iteration 5, loss = 880121472.18037570\n",
      "Iteration 6, loss = 104379090.66234230\n",
      "Iteration 7, loss = 102782165.67654693\n",
      "Iteration 8, loss = 30953599.14499677\n",
      "Iteration 9, loss = 13843132.66198813\n",
      "Iteration 10, loss = 6208237.30346275\n",
      "Iteration 11, loss = 4222803.64718295\n",
      "Iteration 12, loss = 3538294.75004323\n",
      "Iteration 13, loss = 2730992.63320717\n",
      "Iteration 14, loss = 2074477.15826302\n",
      "Iteration 15, loss = 1725245.53028252\n",
      "Iteration 16, loss = 1350345.71178087\n",
      "Iteration 17, loss = 1185431.90807181\n",
      "Iteration 18, loss = 925331.67560121\n",
      "Iteration 19, loss = 810170.06047891\n",
      "Iteration 20, loss = 651561.88921188\n",
      "Iteration 21, loss = 518323.93778350\n",
      "Iteration 22, loss = 480130.77395614\n",
      "Iteration 23, loss = 372596.72400601\n",
      "Iteration 24, loss = 303420.94825686\n",
      "Iteration 25, loss = 284268.50156662\n",
      "Iteration 26, loss = 218867.43488972\n",
      "Iteration 27, loss = 178797.53391479\n",
      "Iteration 28, loss = 150700.93503121\n",
      "Iteration 29, loss = 133104.35352881\n",
      "Iteration 30, loss = 105517.39750855\n",
      "Iteration 31, loss = 94231.56041434\n",
      "Iteration 32, loss = 74355.06758147\n",
      "Iteration 33, loss = 66223.87699042\n",
      "Iteration 34, loss = 52530.83018726\n",
      "Iteration 35, loss = 47653.51377744\n",
      "Iteration 36, loss = 36131.97923202\n",
      "Iteration 37, loss = 31874.46246433\n",
      "Iteration 38, loss = 25972.13744216\n",
      "Iteration 39, loss = 21048.82181407\n",
      "Iteration 40, loss = 17980.87379394\n",
      "Iteration 41, loss = 14535.63914378\n",
      "Iteration 42, loss = 12341.17382355\n",
      "Iteration 43, loss = 11367.22428816\n",
      "Iteration 44, loss = 9972.95825409\n",
      "Iteration 45, loss = 8461.35680907\n",
      "Iteration 46, loss = 20420.95927851\n",
      "Iteration 47, loss = 14090.46036087\n",
      "Iteration 48, loss = 6026.56389488\n",
      "Iteration 49, loss = 4874.57528174\n",
      "Iteration 50, loss = 4843.55500749\n",
      "Iteration 51, loss = 3795.58992497\n",
      "Iteration 52, loss = 3904.68094153\n",
      "Iteration 53, loss = 2942.99605744\n",
      "Iteration 54, loss = 2817.98004356\n",
      "Iteration 55, loss = 2709.17584233\n",
      "Iteration 56, loss = 3740.28716511\n",
      "Iteration 57, loss = 3988.49550784\n",
      "Iteration 58, loss = 2880.56595895\n",
      "Iteration 59, loss = 2094.52634564\n",
      "Iteration 60, loss = 2220.33685922\n",
      "Iteration 61, loss = 1753.42687593\n",
      "Iteration 62, loss = 1898.30451536\n",
      "Iteration 63, loss = 1724.51608728\n",
      "Iteration 64, loss = 1763.66929045\n",
      "Iteration 65, loss = 2245.16090406\n",
      "Iteration 66, loss = 1789.70856151\n",
      "Iteration 67, loss = 2633.58756413\n",
      "Iteration 68, loss = 1422.23661907\n",
      "Iteration 69, loss = 1753.87186051\n",
      "Iteration 70, loss = 1601.14529565\n",
      "Iteration 71, loss = 1343.14350842\n",
      "Iteration 72, loss = 1934.83811675\n",
      "Iteration 73, loss = 2990.81863803\n",
      "Iteration 74, loss = 1387.22944633\n",
      "Iteration 75, loss = 1265.76599085\n",
      "Iteration 76, loss = 1358.84214286\n",
      "Iteration 77, loss = 1290.64526235\n",
      "Iteration 78, loss = 1162.06404743\n",
      "Iteration 79, loss = 1136.25635937\n",
      "Iteration 80, loss = 1133.56025471\n",
      "Iteration 81, loss = 1154.64277584\n",
      "Iteration 82, loss = 1097.54089930\n",
      "Iteration 83, loss = 1136.73297687\n",
      "Iteration 84, loss = 1170.97517407\n",
      "Iteration 85, loss = 1120.48263411\n",
      "Iteration 86, loss = 1005.45945030\n",
      "Iteration 87, loss = 993.68544169\n",
      "Iteration 88, loss = 1101.11970943\n",
      "Iteration 89, loss = 1042.31016431\n",
      "Iteration 90, loss = 962.08487011\n",
      "Iteration 91, loss = 1317.13794763\n",
      "Iteration 92, loss = 1298.92463496\n",
      "Iteration 93, loss = 977.11566198\n",
      "Iteration 94, loss = 977.15707592\n",
      "Iteration 95, loss = 983.80202125\n",
      "Iteration 96, loss = 866.85219600\n",
      "Iteration 97, loss = 1069.96365512\n",
      "Iteration 98, loss = 2193.14571449\n",
      "Iteration 99, loss = 1131.56963396\n",
      "Iteration 100, loss = 946.95627710\n",
      "Iteration 101, loss = 877.75325702\n",
      "Iteration 102, loss = 1308.07275508\n",
      "Iteration 103, loss = 923.79149680\n",
      "Iteration 104, loss = 1957.39544640\n",
      "Iteration 105, loss = 1264.38378527\n",
      "Iteration 106, loss = 944.50894338\n",
      "Iteration 107, loss = 956.13111648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(learning_rate_init=0.1, max_iter=500, verbose=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anetwork =  MLPRegressor(solver='adam', activation='relu', max_iter=500, verbose=True, learning_rate_init=0.1)\n",
    "anetwork.fit(x_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 92600713.51746923\n",
      "Iteration 2, loss = 13660591.39796443\n",
      "Iteration 3, loss = 1218036.34627504\n",
      "Iteration 4, loss = 2663947.57902106\n",
      "Iteration 5, loss = 1887934.98291686\n",
      "Iteration 6, loss = 1077017.71525194\n",
      "Iteration 7, loss = 626464.87830346\n",
      "Iteration 8, loss = 589221.76671558\n",
      "Iteration 9, loss = 425600.05002692\n",
      "Iteration 10, loss = 497262.52167385\n",
      "Iteration 11, loss = 875542.72220131\n",
      "Iteration 12, loss = 533503.93557471\n",
      "Iteration 13, loss = 1701651.94498644\n",
      "Iteration 14, loss = 5055868.12282334\n",
      "Iteration 15, loss = 18394904.13003911\n",
      "Iteration 16, loss = 4905083.73060472\n",
      "Iteration 17, loss = 6679252.07985509\n",
      "Iteration 18, loss = 17368454.47818723\n",
      "Iteration 19, loss = 37856458.84900524\n",
      "Iteration 20, loss = 49558404.05316364\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='identity', hidden_layer_sizes=(100, 100, 100, 50, 50),\n",
       "             learning_rate_init=0.0001, max_iter=500, verbose=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnetwork = MLPRegressor(solver='adam', activation='identity', max_iter=500, verbose=True, learning_rate_init=0.0001)\n",
    "bnetwork.fit(x_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 682.56950589\n",
      "Iteration 2, loss = 659.98204911\n",
      "Iteration 3, loss = 648.45795387\n",
      "Iteration 4, loss = 642.11646851\n",
      "Iteration 5, loss = 637.72697637\n",
      "Iteration 6, loss = 634.41171551\n",
      "Iteration 7, loss = 631.69354863\n",
      "Iteration 8, loss = 629.84864217\n",
      "Iteration 9, loss = 628.30276795\n",
      "Iteration 10, loss = 626.54747444\n",
      "Iteration 11, loss = 620.48798196\n",
      "Iteration 12, loss = 614.06821906\n",
      "Iteration 13, loss = 609.52638245\n",
      "Iteration 14, loss = 604.71660772\n",
      "Iteration 15, loss = 599.81051481\n",
      "Iteration 16, loss = 596.25754709\n",
      "Iteration 17, loss = 592.92519618\n",
      "Iteration 18, loss = 587.76194021\n",
      "Iteration 19, loss = 583.49729831\n",
      "Iteration 20, loss = 579.94888697\n",
      "Iteration 21, loss = 576.51761750\n",
      "Iteration 22, loss = 574.17066200\n",
      "Iteration 23, loss = 569.82664732\n",
      "Iteration 24, loss = 567.66404477\n",
      "Iteration 25, loss = 565.10115598\n",
      "Iteration 26, loss = 562.32351143\n",
      "Iteration 27, loss = 561.37451707\n",
      "Iteration 28, loss = 559.11790796\n",
      "Iteration 29, loss = 553.81355110\n",
      "Iteration 30, loss = 548.12968331\n",
      "Iteration 31, loss = 544.45965478\n",
      "Iteration 32, loss = 540.94529121\n",
      "Iteration 33, loss = 539.07097863\n",
      "Iteration 34, loss = 536.52103509\n",
      "Iteration 35, loss = 535.36862824\n",
      "Iteration 36, loss = 529.96110066\n",
      "Iteration 37, loss = 526.71532988\n",
      "Iteration 38, loss = 525.94092253\n",
      "Iteration 39, loss = 524.31687050\n",
      "Iteration 40, loss = 519.00816810\n",
      "Iteration 41, loss = 512.76227509\n",
      "Iteration 42, loss = 514.67694411\n",
      "Iteration 43, loss = 508.71813024\n",
      "Iteration 44, loss = 506.13242524\n",
      "Iteration 45, loss = 502.59368730\n",
      "Iteration 46, loss = 500.39564285\n",
      "Iteration 47, loss = 502.36545612\n",
      "Iteration 48, loss = 500.19058084\n",
      "Iteration 49, loss = 497.28260506\n",
      "Iteration 50, loss = 495.37154221\n",
      "Iteration 51, loss = 494.16455291\n",
      "Iteration 52, loss = 491.68279054\n",
      "Iteration 53, loss = 489.43650228\n",
      "Iteration 54, loss = 488.10583791\n",
      "Iteration 55, loss = 486.08958498\n",
      "Iteration 56, loss = 483.24438897\n",
      "Iteration 57, loss = 481.09426982\n",
      "Iteration 58, loss = 479.18900019\n",
      "Iteration 59, loss = 479.58240580\n",
      "Iteration 60, loss = 479.98093887\n",
      "Iteration 61, loss = 479.47231351\n",
      "Iteration 62, loss = 477.78476205\n",
      "Iteration 63, loss = 476.90079903\n",
      "Iteration 64, loss = 475.31745252\n",
      "Iteration 65, loss = 474.84123793\n",
      "Iteration 66, loss = 472.01468280\n",
      "Iteration 67, loss = 473.01970690\n",
      "Iteration 68, loss = 472.76593038\n",
      "Iteration 69, loss = 472.63407398\n",
      "Iteration 70, loss = 475.15051100\n",
      "Iteration 71, loss = 471.53578161\n",
      "Iteration 72, loss = 468.97139342\n",
      "Iteration 73, loss = 466.40254302\n",
      "Iteration 74, loss = 466.92110140\n",
      "Iteration 75, loss = 465.95118122\n",
      "Iteration 76, loss = 462.93587541\n",
      "Iteration 77, loss = 462.06714751\n",
      "Iteration 78, loss = 460.08915875\n",
      "Iteration 79, loss = 456.90407847\n",
      "Iteration 80, loss = 456.40009135\n",
      "Iteration 81, loss = 455.74971836\n",
      "Iteration 82, loss = 455.37838967\n",
      "Iteration 83, loss = 454.81093078\n",
      "Iteration 84, loss = 452.63201113\n",
      "Iteration 85, loss = 452.83142584\n",
      "Iteration 86, loss = 454.16731071\n",
      "Iteration 87, loss = 450.66011272\n",
      "Iteration 88, loss = 449.12776221\n",
      "Iteration 89, loss = 449.24342830\n",
      "Iteration 90, loss = 451.71000372\n",
      "Iteration 91, loss = 455.98933684\n",
      "Iteration 92, loss = 453.13529067\n",
      "Iteration 93, loss = 451.58918703\n",
      "Iteration 94, loss = 450.06773942\n",
      "Iteration 95, loss = 448.17906765\n",
      "Iteration 96, loss = 447.28993275\n",
      "Iteration 97, loss = 445.89148057\n",
      "Iteration 98, loss = 445.01108011\n",
      "Iteration 99, loss = 444.92638226\n",
      "Iteration 100, loss = 442.18708636\n",
      "Iteration 101, loss = 441.42911520\n",
      "Iteration 102, loss = 438.84152215\n",
      "Iteration 103, loss = 439.82378377\n",
      "Iteration 104, loss = 438.14577650\n",
      "Iteration 105, loss = 437.16011146\n",
      "Iteration 106, loss = 435.16469621\n",
      "Iteration 107, loss = 436.29544304\n",
      "Iteration 108, loss = 433.80121340\n",
      "Iteration 109, loss = 437.12472590\n",
      "Iteration 110, loss = 435.01784800\n",
      "Iteration 111, loss = 433.10325631\n",
      "Iteration 112, loss = 430.25277196\n",
      "Iteration 113, loss = 428.38635358\n",
      "Iteration 114, loss = 426.52611555\n",
      "Iteration 115, loss = 424.18964512\n",
      "Iteration 116, loss = 424.99775797\n",
      "Iteration 117, loss = 423.20295765\n",
      "Iteration 118, loss = 422.20393537\n",
      "Iteration 119, loss = 421.51953295\n",
      "Iteration 120, loss = 425.83857830\n",
      "Iteration 121, loss = 421.50239679\n",
      "Iteration 122, loss = 420.47749584\n",
      "Iteration 123, loss = 419.84358586\n",
      "Iteration 124, loss = 424.60822137\n",
      "Iteration 125, loss = 415.93921184\n",
      "Iteration 126, loss = 417.50707994\n",
      "Iteration 127, loss = 418.84332374\n",
      "Iteration 128, loss = 419.90369384\n",
      "Iteration 129, loss = 417.99360095\n",
      "Iteration 130, loss = 416.69493284\n",
      "Iteration 131, loss = 415.32547744\n",
      "Iteration 132, loss = 412.32516190\n",
      "Iteration 133, loss = 410.36481863\n",
      "Iteration 134, loss = 408.73100997\n",
      "Iteration 135, loss = 408.62618124\n",
      "Iteration 136, loss = 408.60334430\n",
      "Iteration 137, loss = 406.74750951\n",
      "Iteration 138, loss = 406.03575396\n",
      "Iteration 139, loss = 404.27166942\n",
      "Iteration 140, loss = 402.59422303\n",
      "Iteration 141, loss = 401.27697273\n",
      "Iteration 142, loss = 412.58750436\n",
      "Iteration 143, loss = 414.01941575\n",
      "Iteration 144, loss = 407.25958269\n",
      "Iteration 145, loss = 403.96839469\n",
      "Iteration 146, loss = 403.27925736\n",
      "Iteration 147, loss = 404.36289324\n",
      "Iteration 148, loss = 403.10395955\n",
      "Iteration 149, loss = 404.20927294\n",
      "Iteration 150, loss = 401.16877331\n",
      "Iteration 151, loss = 397.55839453\n",
      "Iteration 152, loss = 397.19267271\n",
      "Iteration 153, loss = 396.54178772\n",
      "Iteration 154, loss = 397.70507782\n",
      "Iteration 155, loss = 399.46351924\n",
      "Iteration 156, loss = 400.87394131\n",
      "Iteration 157, loss = 399.20609072\n",
      "Iteration 158, loss = 398.67842904\n",
      "Iteration 159, loss = 398.33047859\n",
      "Iteration 160, loss = 397.50602581\n",
      "Iteration 161, loss = 397.57756802\n",
      "Iteration 162, loss = 396.11853044\n",
      "Iteration 163, loss = 399.28848839\n",
      "Iteration 164, loss = 395.94444065\n",
      "Iteration 165, loss = 397.41813211\n",
      "Iteration 166, loss = 389.87171890\n",
      "Iteration 167, loss = 386.91791383\n",
      "Iteration 168, loss = 385.59819142\n",
      "Iteration 169, loss = 387.27233555\n",
      "Iteration 170, loss = 386.28016329\n",
      "Iteration 171, loss = 384.33078917\n",
      "Iteration 172, loss = 384.72312451\n",
      "Iteration 173, loss = 384.34197438\n",
      "Iteration 174, loss = 384.67778523\n",
      "Iteration 175, loss = 382.31122658\n",
      "Iteration 176, loss = 381.28459105\n",
      "Iteration 177, loss = 384.45929952\n",
      "Iteration 178, loss = 384.42062660\n",
      "Iteration 179, loss = 379.44407115\n",
      "Iteration 180, loss = 381.01346726\n",
      "Iteration 181, loss = 385.35197415\n",
      "Iteration 182, loss = 384.25428079\n",
      "Iteration 183, loss = 387.43417064\n",
      "Iteration 184, loss = 381.25420330\n",
      "Iteration 185, loss = 380.41668303\n",
      "Iteration 186, loss = 376.66885537\n",
      "Iteration 187, loss = 380.07759092\n",
      "Iteration 188, loss = 378.58580951\n",
      "Iteration 189, loss = 375.93161436\n",
      "Iteration 190, loss = 379.40840888\n",
      "Iteration 191, loss = 378.24787100\n",
      "Iteration 192, loss = 377.80878094\n",
      "Iteration 193, loss = 383.49484938\n",
      "Iteration 194, loss = 379.12887328\n",
      "Iteration 195, loss = 376.36393287\n",
      "Iteration 196, loss = 376.20762603\n",
      "Iteration 197, loss = 378.28276907\n",
      "Iteration 198, loss = 378.98650161\n",
      "Iteration 199, loss = 377.78518785\n",
      "Iteration 200, loss = 377.13817922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', hidden_layer_sizes=(200, 100, 100),\n",
       "             max_iter=500, verbose=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnetwork = MLPRegressor(hidden_layer_sizes=(200,100,100,), solver='adam', activation='logistic', max_iter=500, verbose=True)\n",
    "cnetwork.fit(x_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 647.04865831\n",
      "Iteration 2, loss = 607.38178453\n",
      "Iteration 3, loss = 589.08519745\n",
      "Iteration 4, loss = 577.41053635\n",
      "Iteration 5, loss = 571.63796637\n",
      "Iteration 6, loss = 563.81261969\n",
      "Iteration 7, loss = 556.73795298\n",
      "Iteration 8, loss = 546.02821204\n",
      "Iteration 9, loss = 541.08766585\n",
      "Iteration 10, loss = 534.68524675\n",
      "Iteration 11, loss = 529.39201082\n",
      "Iteration 12, loss = 526.13532547\n",
      "Iteration 13, loss = 522.51495731\n",
      "Iteration 14, loss = 517.20829737\n",
      "Iteration 15, loss = 514.20344065\n",
      "Iteration 16, loss = 509.04814489\n",
      "Iteration 17, loss = 504.74142219\n",
      "Iteration 18, loss = 500.59174907\n",
      "Iteration 19, loss = 497.78537721\n",
      "Iteration 20, loss = 489.48634381\n",
      "Iteration 21, loss = 487.99533775\n",
      "Iteration 22, loss = 480.34239682\n",
      "Iteration 23, loss = 487.97978286\n",
      "Iteration 24, loss = 482.55762776\n",
      "Iteration 25, loss = 480.21359125\n",
      "Iteration 26, loss = 480.20685088\n",
      "Iteration 27, loss = 482.67396575\n",
      "Iteration 28, loss = 472.81460361\n",
      "Iteration 29, loss = 470.10575195\n",
      "Iteration 30, loss = 470.48925611\n",
      "Iteration 31, loss = 468.28626508\n",
      "Iteration 32, loss = 466.31690647\n",
      "Iteration 33, loss = 468.14397858\n",
      "Iteration 34, loss = 464.88527302\n",
      "Iteration 35, loss = 464.05911636\n",
      "Iteration 36, loss = 473.47312399\n",
      "Iteration 37, loss = 457.78554991\n",
      "Iteration 38, loss = 462.99784308\n",
      "Iteration 39, loss = 462.38002556\n",
      "Iteration 40, loss = 458.74833327\n",
      "Iteration 41, loss = 449.10210885\n",
      "Iteration 42, loss = 447.99185784\n",
      "Iteration 43, loss = 446.84180500\n",
      "Iteration 44, loss = 452.70332731\n",
      "Iteration 45, loss = 450.19775085\n",
      "Iteration 46, loss = 445.45941467\n",
      "Iteration 47, loss = 444.78241228\n",
      "Iteration 48, loss = 437.48413457\n",
      "Iteration 49, loss = 437.70163476\n",
      "Iteration 50, loss = 442.78327815\n",
      "Iteration 51, loss = 447.56499095\n",
      "Iteration 52, loss = 437.37644004\n",
      "Iteration 53, loss = 430.81237782\n",
      "Iteration 54, loss = 427.35532032\n",
      "Iteration 55, loss = 429.13087491\n",
      "Iteration 56, loss = 428.53073970\n",
      "Iteration 57, loss = 424.17454916\n",
      "Iteration 58, loss = 427.09911568\n",
      "Iteration 59, loss = 423.85525314\n",
      "Iteration 60, loss = 422.96190250\n",
      "Iteration 61, loss = 428.97829961\n",
      "Iteration 62, loss = 428.02542473\n",
      "Iteration 63, loss = 427.47034896\n",
      "Iteration 64, loss = 428.45085465\n",
      "Iteration 65, loss = 426.71606909\n",
      "Iteration 66, loss = 426.76952467\n",
      "Iteration 67, loss = 422.46546631\n",
      "Iteration 68, loss = 419.29632962\n",
      "Iteration 69, loss = 418.48636748\n",
      "Iteration 70, loss = 418.97583774\n",
      "Iteration 71, loss = 416.52705089\n",
      "Iteration 72, loss = 414.32946761\n",
      "Iteration 73, loss = 414.00113316\n",
      "Iteration 74, loss = 411.15003299\n",
      "Iteration 75, loss = 407.52265584\n",
      "Iteration 76, loss = 404.99277888\n",
      "Iteration 77, loss = 403.08502130\n",
      "Iteration 78, loss = 403.51460637\n",
      "Iteration 79, loss = 401.77133760\n",
      "Iteration 80, loss = 403.80846884\n",
      "Iteration 81, loss = 404.47080137\n",
      "Iteration 82, loss = 395.82692081\n",
      "Iteration 83, loss = 396.54569895\n",
      "Iteration 84, loss = 397.57340039\n",
      "Iteration 85, loss = 399.70013254\n",
      "Iteration 86, loss = 400.09192912\n",
      "Iteration 87, loss = 394.36366102\n",
      "Iteration 88, loss = 386.19777531\n",
      "Iteration 89, loss = 387.50574300\n",
      "Iteration 90, loss = 387.61933685\n",
      "Iteration 91, loss = 382.52474372\n",
      "Iteration 92, loss = 377.37905788\n",
      "Iteration 93, loss = 378.57098923\n",
      "Iteration 94, loss = 379.75346180\n",
      "Iteration 95, loss = 371.91360350\n",
      "Iteration 96, loss = 369.65357913\n",
      "Iteration 97, loss = 369.30158544\n",
      "Iteration 98, loss = 376.54105734\n",
      "Iteration 99, loss = 375.28398513\n",
      "Iteration 100, loss = 373.13659923\n",
      "Iteration 101, loss = 368.92094436\n",
      "Iteration 102, loss = 367.39677862\n",
      "Iteration 103, loss = 360.32229491\n",
      "Iteration 104, loss = 356.12777286\n",
      "Iteration 105, loss = 358.22602611\n",
      "Iteration 106, loss = 354.70510280\n",
      "Iteration 107, loss = 354.52279490\n",
      "Iteration 108, loss = 353.34910499\n",
      "Iteration 109, loss = 358.15782023\n",
      "Iteration 110, loss = 361.80688915\n",
      "Iteration 111, loss = 356.40998365\n",
      "Iteration 112, loss = 357.06831936\n",
      "Iteration 113, loss = 367.94209497\n",
      "Iteration 114, loss = 368.35773652\n",
      "Iteration 115, loss = 352.30410380\n",
      "Iteration 116, loss = 354.47628424\n",
      "Iteration 117, loss = 346.75854412\n",
      "Iteration 118, loss = 343.23302871\n",
      "Iteration 119, loss = 338.68118260\n",
      "Iteration 120, loss = 350.70160717\n",
      "Iteration 121, loss = 351.62688161\n",
      "Iteration 122, loss = 348.29012620\n",
      "Iteration 123, loss = 343.21342544\n",
      "Iteration 124, loss = 348.56457536\n",
      "Iteration 125, loss = 342.95520530\n",
      "Iteration 126, loss = 338.96864591\n",
      "Iteration 127, loss = 332.94991632\n",
      "Iteration 128, loss = 340.18628279\n",
      "Iteration 129, loss = 334.07216043\n",
      "Iteration 130, loss = 337.35831718\n",
      "Iteration 131, loss = 330.52518560\n",
      "Iteration 132, loss = 323.19518810\n",
      "Iteration 133, loss = 325.89631018\n",
      "Iteration 134, loss = 329.34745830\n",
      "Iteration 135, loss = 323.51133569\n",
      "Iteration 136, loss = 321.41473523\n",
      "Iteration 137, loss = 316.81458221\n",
      "Iteration 138, loss = 324.75256925\n",
      "Iteration 139, loss = 325.40154600\n",
      "Iteration 140, loss = 330.84386647\n",
      "Iteration 141, loss = 319.54886985\n",
      "Iteration 142, loss = 312.85330196\n",
      "Iteration 143, loss = 309.94477113\n",
      "Iteration 144, loss = 315.39300306\n",
      "Iteration 145, loss = 310.18539950\n",
      "Iteration 146, loss = 306.96360296\n",
      "Iteration 147, loss = 309.11795591\n",
      "Iteration 148, loss = 312.44102092\n",
      "Iteration 149, loss = 313.79564018\n",
      "Iteration 150, loss = 318.32517874\n",
      "Iteration 151, loss = 322.92185494\n",
      "Iteration 152, loss = 308.72159196\n",
      "Iteration 153, loss = 302.97378262\n",
      "Iteration 154, loss = 305.33723128\n",
      "Iteration 155, loss = 302.49032008\n",
      "Iteration 156, loss = 307.73856431\n",
      "Iteration 157, loss = 302.07084541\n",
      "Iteration 158, loss = 313.59469288\n",
      "Iteration 159, loss = 304.83482286\n",
      "Iteration 160, loss = 298.76009865\n",
      "Iteration 161, loss = 297.85934966\n",
      "Iteration 162, loss = 293.52948874\n",
      "Iteration 163, loss = 295.59236412\n",
      "Iteration 164, loss = 292.51313062\n",
      "Iteration 165, loss = 294.52636033\n",
      "Iteration 166, loss = 290.38216908\n",
      "Iteration 167, loss = 288.67116637\n",
      "Iteration 168, loss = 298.40265889\n",
      "Iteration 169, loss = 307.76379496\n",
      "Iteration 170, loss = 305.18701266\n",
      "Iteration 171, loss = 296.61985143\n",
      "Iteration 172, loss = 297.72901039\n",
      "Iteration 173, loss = 292.34424895\n",
      "Iteration 174, loss = 282.30905423\n",
      "Iteration 175, loss = 286.65212691\n",
      "Iteration 176, loss = 286.31165844\n",
      "Iteration 177, loss = 285.58923316\n",
      "Iteration 178, loss = 289.62465125\n",
      "Iteration 179, loss = 282.35117497\n",
      "Iteration 180, loss = 282.57399559\n",
      "Iteration 181, loss = 287.59153868\n",
      "Iteration 182, loss = 283.16239968\n",
      "Iteration 183, loss = 277.79553569\n",
      "Iteration 184, loss = 280.64732669\n",
      "Iteration 185, loss = 278.19473195\n",
      "Iteration 186, loss = 281.15743368\n",
      "Iteration 187, loss = 289.39246491\n",
      "Iteration 188, loss = 280.89016770\n",
      "Iteration 189, loss = 274.64769370\n",
      "Iteration 190, loss = 276.32856613\n",
      "Iteration 191, loss = 271.40732594\n",
      "Iteration 192, loss = 292.74116662\n",
      "Iteration 193, loss = 297.75403846\n",
      "Iteration 194, loss = 276.29334524\n",
      "Iteration 195, loss = 273.40764812\n",
      "Iteration 196, loss = 272.17690609\n",
      "Iteration 197, loss = 264.84881205\n",
      "Iteration 198, loss = 270.78917191\n",
      "Iteration 199, loss = 278.78137236\n",
      "Iteration 200, loss = 263.86810520\n",
      "Iteration 201, loss = 263.02836094\n",
      "Iteration 202, loss = 304.33431677\n",
      "Iteration 203, loss = 292.24748840\n",
      "Iteration 204, loss = 270.88952215\n",
      "Iteration 205, loss = 266.10097548\n",
      "Iteration 206, loss = 265.87949804\n",
      "Iteration 207, loss = 264.76415894\n",
      "Iteration 208, loss = 262.88445441\n",
      "Iteration 209, loss = 261.66978968\n",
      "Iteration 210, loss = 265.81998923\n",
      "Iteration 211, loss = 278.14848356\n",
      "Iteration 212, loss = 313.17084260\n",
      "Iteration 213, loss = 275.27696425\n",
      "Iteration 214, loss = 264.96832167\n",
      "Iteration 215, loss = 257.31281156\n",
      "Iteration 216, loss = 256.59491952\n",
      "Iteration 217, loss = 271.09394319\n",
      "Iteration 218, loss = 260.08303790\n",
      "Iteration 219, loss = 266.12807606\n",
      "Iteration 220, loss = 257.51211597\n",
      "Iteration 221, loss = 258.74824239\n",
      "Iteration 222, loss = 252.51572609\n",
      "Iteration 223, loss = 252.24743063\n",
      "Iteration 224, loss = 252.79413654\n",
      "Iteration 225, loss = 247.83842288\n",
      "Iteration 226, loss = 250.99350567\n",
      "Iteration 227, loss = 252.52000474\n",
      "Iteration 228, loss = 264.66878705\n",
      "Iteration 229, loss = 263.51183091\n",
      "Iteration 230, loss = 259.37391551\n",
      "Iteration 231, loss = 248.96316009\n",
      "Iteration 232, loss = 247.30742266\n",
      "Iteration 233, loss = 248.56598835\n",
      "Iteration 234, loss = 247.04536444\n",
      "Iteration 235, loss = 247.93202260\n",
      "Iteration 236, loss = 247.11208531\n",
      "Iteration 237, loss = 249.85751001\n",
      "Iteration 238, loss = 243.18932593\n",
      "Iteration 239, loss = 241.62150780\n",
      "Iteration 240, loss = 243.01301118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 241, loss = 241.98441577\n",
      "Iteration 242, loss = 241.29605764\n",
      "Iteration 243, loss = 246.38624364\n",
      "Iteration 244, loss = 244.17157649\n",
      "Iteration 245, loss = 253.16812075\n",
      "Iteration 246, loss = 246.76578859\n",
      "Iteration 247, loss = 257.70095796\n",
      "Iteration 248, loss = 243.22144092\n",
      "Iteration 249, loss = 247.93685241\n",
      "Iteration 250, loss = 245.26918120\n",
      "Iteration 251, loss = 243.34774148\n",
      "Iteration 252, loss = 240.59121799\n",
      "Iteration 253, loss = 242.87799561\n",
      "Iteration 254, loss = 237.80098876\n",
      "Iteration 255, loss = 238.27118939\n",
      "Iteration 256, loss = 234.00012470\n",
      "Iteration 257, loss = 233.83879309\n",
      "Iteration 258, loss = 240.52671982\n",
      "Iteration 259, loss = 233.67481158\n",
      "Iteration 260, loss = 245.44692329\n",
      "Iteration 261, loss = 250.76320812\n",
      "Iteration 262, loss = 268.69943913\n",
      "Iteration 263, loss = 291.77035895\n",
      "Iteration 264, loss = 298.15847099\n",
      "Iteration 265, loss = 292.40940893\n",
      "Iteration 266, loss = 266.16933840\n",
      "Iteration 267, loss = 260.75251291\n",
      "Iteration 268, loss = 256.69961175\n",
      "Iteration 269, loss = 259.89582852\n",
      "Iteration 270, loss = 257.03893120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='tanh', hidden_layer_sizes=(200, 100, 100),\n",
       "             max_iter=500, verbose=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnetwork = MLPRegressor(hidden_layer_sizes=(200,100,100,), solver='adam', activation='tanh', max_iter=500, verbose=True)\n",
    "dnetwork.fit(x_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10574477.871004986"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anetwork.score(x_test, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23693072.09502039"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnetwork.score(x_test, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39545085285366344"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnetwork.score(x_test, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4020586783332841"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnetwork.score(x_test, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tanh and logistic activation functions are by far the most accurate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
